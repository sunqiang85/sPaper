{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "本文基于Jacob Devlin的[论文](https://arxiv.org/pdf/1810.04805.pdf)，以及Huggingface在github上的[code](https://github.com/huggingface/pytorch-transformers)，对BERT进行解读\n",
    "\n",
    "\n",
    "BERT使用了Transformer中的encoder部分，以$BERT_{base}$为例，encoder Layer为$L=12$层，multi-attention中的heads为，Hidden Layer为$H=12$，feed-foward的filter size（也可以理解为中间隐含层）的尺寸为$4H=3072$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型的训练任务一：Masked LM\n",
    "对于每一个句子，以15%的概率选出单词，以下面三种方式处理：\n",
    "- 80%的时候，替换成[MASK], 举例: my do is hairy $\\rightarrow$ my dog is [MASK]\n",
    "- 10%的时候，替换成随机单词，举例: my do is hairy $\\rightarrow$ my dog is apple\n",
    "- 10%的时候，保存不变，举例: my do is hairy $\\rightarrow$ my dog is hairy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型的训练任务二：NextSentence\n",
    "判断句子A，句子B之间是否为上下句关系：\n",
    "- 50%的时候，是上下句关系：[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
    "- 50%的时候，不是上下句关系：[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "采用WordPiece embedding, 有30k的词汇量。后缀会分成以'##'开头的片段。特殊词汇定义如下\n",
    "unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\",mask_token=\"[MASK]\"，其中句首为[CLS]，句子分隔符为[SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a',\n",
    "                              'puppet', '##eer', '[SEP]']\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换成单词id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958,   103,  2001,\n",
      "          1037, 13997, 11510,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "得到句子在语言模型中对应的单词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.config.output_hidden_states=True\n",
    "# Set the model in evaluation mode to desactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproductible results during evaluation!\n",
    "model.eval()\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # PyTorch-Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "预测mask单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model有三部分组成\n",
    "- embedding：将句子映射成向量序列：（batch，sequence_length,hidden_size），向量包含了单词，位置，句子序号信息\n",
    "- encoder：通过Transformer的L层encoderLayer，得到L个隐含状态层\n",
    "- pooler：对最后一个状态中句子开头的[CLS]所对应的向量（代码中进行线性变换，tanh并进行输出，论文中为明说）\n",
    "\n",
    "其中forward框架如下：\n",
    "\n",
    "输入:\n",
    " - input_ids: 单词tokenized后的序号\n",
    " - token_type_ids: 就是Segmentation id；句子A、句子B\n",
    " - attention_mask：区分padding和正常单词\n",
    "\n",
    "输出:\n",
    " - sequence_output (batch*sequence_length*hidden_size):包含了句子中每一个单词对应的最后层隐含状态\n",
    " - pooled_output: 第一个单词对应的隐含状态的再次线性变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n",
    "        position_ids=None, head_mask=None):\n",
    "    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # 增加head，query维度\n",
    "    extended_attention_mask = extended_attention_mask.to(\n",
    "        dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "    \n",
    "    # step-1\n",
    "    embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
    "    # step-2\n",
    "    encoder_outputs = self.encoder(embedding_output,\n",
    "                                   extended_attention_mask,\n",
    "                                   head_mask=head_mask)\n",
    "    sequence_output = encoder_outputs[0]\n",
    "    #\n",
    "    pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "    outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
    "    return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  step-1 embedding\n",
    "其中step-1中的embedding又分为了三部分：\n",
    "- WordPiece Embedding: \n",
    "- learned position embedding 同词向量embedding相同。句子最长512。（512*dim）\n",
    "- Segment Embedding：句子1、句子2. （2*dim）\n",
    "![](images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_output = model.embeddings(tokens_tensor, position_ids=None, \n",
    "                                   token_type_ids=segments_tensors)\n",
    "assert tuple(embedding_output.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "encoder中代码摘要\n",
    "```python\n",
    "self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "    embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2 Encoder\n",
    "encoder的结构图如所示，包含了L个encoder Layer，每个layer包含一个Multihead Self-Attention（BertAttention）子层和FeedForward(BertIntermediate+BertOutput)子层。详见Transformer\n",
    "其中BertLayer的结构如下（就是Transformer中的encoder Layer）：\n",
    "- BertAttention: LayerNorm(MultiHeadAttention(x)+x)\n",
    "- BertIntermediate+BertOutput: 就是LayerNorm(FF(x)+x)\n",
    "\n",
    "![](images/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones_like(tokens_tensor)\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=next(model.parameters()).dtype) # fp16 compatibility\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "head_mask = [None] * model.config.num_hidden_layers\n",
    "model.encoder.output_hidden_states=True\n",
    "encoder_outputs = model.encoder(embedding_output,\n",
    "                                       extended_attention_mask,\n",
    "                                       head_mask=head_mask)\n",
    "# encoder_ouputs: outputs, (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertEncoder 代码摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
    "        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        attention_output = attention_outputs[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # outputs, (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step-3 BertPool\n",
    "取每一个句子的第一个单词，进行线性变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_output = encoder_outputs[0]\n",
    "pooled_output = model.pooler(sequence_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_token_tensor = sequence_output[:,0]\n",
    "assert tuple(first_token_tensor.shape) == (1,model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertForMaskedLM\n",
    "包含两个组件\n",
    "- BertModel\n",
    "- BertOnlyMLMHEAD(cls): 其实就是一个包含单隐层的FeedForward网络\n",
    "    - BertLMPredictionHead\n",
    "        - BertPredictionHeadTransform (hidden_size->hidden_size[acitvation])\n",
    "        - decoder（线性映射hidden_size->vocab_size）\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n",
    "            position_ids=None, head_mask=None):\n",
    "    outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                        attention_mask=attention_mask, head_mask=head_mask)\n",
    "\n",
    "    sequence_output = outputs[0]\n",
    "    prediction_scores = self.cls(sequence_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPredictionHeadTransform, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size,\n",
    "                                 config.vocab_size,\n",
    "                                 bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertOnlyMLMHead, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertForNextSentencePrediction流程\n",
    "- BertModel\n",
    "- BertOnlyNSPHead(hidden_size->2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertOnlyNSPHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertOnlyNSPHead, self).__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask, head_mask=head_mask)\n",
    "pooled_output = outputs[1]\n",
    "seq_relationship_score = self.cls(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
