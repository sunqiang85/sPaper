{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word history\n",
    "- 1-of-N Encoding\n",
    "- word class\n",
    "- word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO: Embedding from Language Model\n",
    "- RNN based language model: predict next token\n",
    "- 不同层的hidden layer的输出：在接下游任务的embedding时，使用的权重$\\alpha$是学习得到的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT： Bidirectional Encoder Represenation from transformer\n",
    "- BERT: Encoder of Transformer Learned from a large amount of text without annotation\n",
    "- Classification\n",
    "- Cloze\n",
    "- Extraction-based Question. Output: two integer(s,e)\n",
    "- 每一层学习到的是什么. 从低级到高级, 通过weighted sum来得到\n",
    "- Multi-lingual BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERNIE: Enhanced Representation though knowledge integration (ERNIE)\n",
    "- designed for chinese\n",
    "- mask采用词汇级别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Pre-Training\n",
    "GPT 是Transformer的Decoder\n",
    "- ELMO: 94M\n",
    "- BERT: 340M\n",
    "- GPT-2: 1542M \n",
    "\n",
    "可以做Zero-shot learning\n",
    "- Reading Comprehension:\n",
    "- Summarization\n",
    "- Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
