{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning\n",
    "Meta Learning = Learn to learn。\n",
    "与life-long learning的区别：lief-long learning中是希望学到同一个模型；Meta-learning中是不同的模型。\n",
    "\n",
    "Meta Learning的目标是学习到一个F算法，就是根据数据得到一个函数f能够解决数据代表的任务。传统的机器学习中，设计了网络结构，参数初始化，更新规则后，就是一个F学习算法，根据数据，就能得到一个模型f。\n",
    "\n",
    "任何对一个学习算法集合中某一个F的好坏。\n",
    "\n",
    "Meta Learning的输入是Tasks，分为Training Task; Testing Tasks; 其中Training Task中分为Support Set和Query Set。\n",
    "\n",
    "Defining the goodness of a function:\n",
    "$$\n",
    "L(F)=\\sum_{n=1}^N l^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML\n",
    "- Chelsea Finn, Sergey Levine, \"Model-Agnostic Meta-Learning for Fast Adaption of Deep Networks\"\n",
    "学习一个最好的初始化参数，只关注初始化参数$\\Phi$\n",
    "\n",
    "- MAML与Model Pretraining的区别：MAML关注的是选择$\\Phi$在after training后loss比较小\n",
    "\n",
    "- Training\n",
    "$$\n",
    "\\Phi \\leftarrow \\Phi - \\eta \\nabla_{\\Phi}L(\\Phi) \n",
    "$$\n",
    "在实际应用中做了一阶近似\n",
    "$$\n",
    "\\nabla_{\\Phi}L(\\Phi)=\\nabla_{\\Phi} \\sum_{i=1}^n l^n(\\hat{\\theta}) \\approx \\sum_{i=1}^n \\frac{\\partial{l(\\hat{\\theta})}}{\\partial{\\hat{\\theta}_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reptile\n",
    "可以训练多步，每步都朝最终的$\\hat{\\theta_n}$移动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More\n",
    "- 初始参数：MAML，Reptile\n",
    "- 模型结构：可以通过RL\n",
    "- 更新规则：Gradient Descent as a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent as a LSTM\n",
    "- Sachin Ravi, Optimization as a model for few-shot learning\n",
    "- Marcin Andrychowicz, Learning to learn by gradient descent by gradient descent\n",
    "\n",
    "将LSTM的状态c中存储参数$\\theta$,输入x是负的梯度，LSTM可以看做是Gradient Descent更General的一个版本。\n",
    "\n",
    "实现技巧：LSTM中c只保存一个值，每个参数都分别通过同一个LSTM一轮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Based Approach\n",
    "直接进行Learning+Prediction。\n",
    "\n",
    "Few-Shot Learning：face veryfication\n",
    "\n",
    "- Siamese Network：通过CNN学习得到两个embedding，计算两个embedding之间的相似性\n",
    "- prototypical Network：使用CNN计算embedding，然后计算相似度； Multiple hop like memory network\n",
    "- Matching Network：使用了Bidirectional LSTM计算embedding；\n",
    "- Relation Network：同prototypical类似，先计算embedding，相似度是通过一个神经网络来学习得到，输入是两个embedding的拼接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning for Imaginary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train + Test as RNN\n",
    "- MANN: Memory Augmented Neural Network\n",
    "- SNAIL: A Simple Neural Attentive Meta-Learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
